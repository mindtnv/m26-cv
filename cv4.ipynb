{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Обнаружение и сегментация объектов\n",
    "\n",
    "Антонов Михаил Евгеньевич, М-26"
   ],
   "metadata": {
    "id": "TAYuSZd-G3BI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Часть 1: Метрика IoU и алгоритм NMS\n",
    "\n",
    "### 1.1. Intersection over Union (IoU)\n",
    "\n",
    "**Определение:** IoU (Intersection over Union) - метрика для оценки пересечения двух bounding box'ов. Вычисляется как отношение площади пересечения к площади объединения."
   ],
   "metadata": {
    "id": "BjbjJrSTHQLg"
   }
  },
  {
   "cell_type": "code",
   "source": "from typing import List, Tuple, Dict, Optional\nfrom io import BytesIO\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport requests\nimport torch\nimport torchvision\nfrom torchvision import transforms as T\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn, maskrcnn_resnet50_fpn\nfrom torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights, MaskRCNN_ResNet50_FPN_Weights\nfrom torchvision.models.segmentation import deeplabv3_resnet101\nfrom torchvision.models.segmentation import DeepLabV3_ResNet101_Weights\nfrom PIL import Image\n\n# Константы\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nDEFAULT_CONFIDENCE_THRESHOLD = 0.5\nDEFAULT_IOU_THRESHOLD = 0.5\n\n# Названия классов COCO\nCOCO_CATEGORIES = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n    'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat',\n    'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass',\n    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n    'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator',\n    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\n\ndef calculate_iou(box1: List[float], box2: List[float]) -> float:\n    \"\"\"Вычисляет IoU (Intersection over Union) для двух bounding box.\n    \n    Args:\n        box1: Первый box в формате [x1, y1, x2, y2]\n        box2: Второй box в формате [x1, y1, x2, y2]\n        \n    Returns:\n        Значение IoU от 0 до 1\n    \"\"\"\n    x1_1, y1_1, x2_1, y2_1 = box1\n    x1_2, y1_2, x2_2, y2_2 = box2\n\n    # Координаты пересечения\n    x_left = max(x1_1, x1_2)\n    y_top = max(y1_1, y1_2)\n    x_right = min(x2_1, x2_2)\n    y_bottom = min(y2_1, y2_2)\n\n    # Проверка на отсутствие пересечения\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    intersection = (x_right - x_left) * (y_bottom - y_top)\n    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n    union = area1 + area2 - intersection\n\n    return intersection / union if union > 0 else 0.0\n\n\ndef non_max_suppression(\n    boxes: np.ndarray,\n    scores: np.ndarray,\n    iou_threshold: float = DEFAULT_IOU_THRESHOLD\n) -> List[int]:\n    \"\"\"Применяет Non-Maximum Suppression к набору bounding boxes.\n    \n    Args:\n        boxes: Массив boxes формы (N, 4) в формате [x1, y1, x2, y2]\n        scores: Массив confidence scores формы (N,)\n        iou_threshold: Порог IoU для подавления\n        \n    Returns:\n        Список индексов оставшихся boxes\n    \"\"\"\n    boxes = np.array(boxes)\n    scores = np.array(scores)\n\n    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n    areas = (x2 - x1) * (y2 - y1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(int(i))\n\n        # Вычисление IoU с оставшимися boxes\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        intersection = w * h\n        union = areas[i] + areas[order[1:]] - intersection\n        iou = intersection / union\n\n        # Оставляем только boxes с IoU ниже порога\n        inds = np.where(iou <= iou_threshold)[0]\n        order = order[inds + 1]\n\n    return keep\n\n\n# Демонстрация IoU\nprint(\"Тестирование функции IoU:\")\nprint(\"=\" * 40)\n\ntest_cases = [\n    ([10, 10, 50, 50], [10, 10, 50, 50], \"Полное перекрытие\"),\n    ([10, 10, 50, 50], [20, 20, 60, 60], \"Частичное перекрытие\"),\n    ([10, 10, 50, 50], [70, 70, 100, 100], \"Нет перекрытия\"),\n]\n\nfor box1, box2, description in test_cases:\n    iou = calculate_iou(box1, box2)\n    print(f\"{description}: IoU = {iou:.3f}\")\n\n# Визуализация IoU\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\ncolors = ['red', 'blue']\n\nfor idx, (box1, box2, title) in enumerate(test_cases):\n    ax = axes[idx]\n    iou = calculate_iou(box1, box2)\n    \n    for i, box in enumerate([box1, box2]):\n        x1, y1, x2, y2 = box\n        rect = patches.Rectangle(\n            (x1, y1), x2 - x1, y2 - y1,\n            linewidth=2, edgecolor=colors[i], facecolor='none',\n            alpha=0.7, label=f'Box {i+1}'\n        )\n        ax.add_patch(rect)\n\n    ax.set_xlim(0, 110)\n    ax.set_ylim(0, 110)\n    ax.set_title(f\"{title}\\nIoU = {iou:.3f}\")\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.grid(True, alpha=0.3)\n    ax.legend()\n\nplt.tight_layout()\nplt.show()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "TOBaP7gzHT0q",
    "outputId": "9a38085a-8d1f-4bf2-a073-5ee206c00511"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Демонстрация NMS\nprint(\"Тестирование Non-Maximum Suppression:\")\nprint(\"=\" * 50)\n\ntest_boxes = [\n    [20, 20, 80, 80],\n    [30, 30, 90, 90],\n    [25, 25, 85, 85],\n    [100, 100, 150, 150],\n    [110, 110, 160, 160],\n    [200, 200, 250, 250],\n]\ntest_scores = [0.9, 0.8, 0.7, 0.85, 0.75, 0.95]\n\nprint(\"Исходные bounding boxes:\")\nfor i, (box, score) in enumerate(zip(test_boxes, test_scores)):\n    print(f\"  Box {i+1}: {box}, score={score:.2f}\")\n\n# Тестирование с разными порогами\nfor threshold in [0.3, 0.5, 0.7]:\n    selected = non_max_suppression(test_boxes, test_scores, iou_threshold=threshold)\n    print(f\"\\nNMS (IoU threshold={threshold}):\")\n    print(f\"  Оставшиеся boxes: {[i+1 for i in selected]}\")\n    for idx in selected:\n        print(f\"    Box {idx+1}: {test_boxes[idx]}, score={test_scores[idx]:.2f}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9wv7LWyPH-Qq",
    "outputId": "e5e2f3b9-7bdb-488a-f2ef-a769a40764a4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Визуализация NMS на примере перекрывающихся прямоугольников\ndemo_boxes = [\n    [30, 30, 100, 100],\n    [50, 50, 120, 120],\n    [40, 40, 110, 110],\n    [70, 70, 140, 140],\n]\ndemo_scores = [0.85, 0.92, 0.78, 0.88]\n\nprint(\"Демонстрация NMS на перекрывающихся прямоугольниках:\")\nprint(\"-\" * 50)\n\n# Расчёт IoU между всеми парами\nprint(\"\\nМатрица IoU:\")\nfor i in range(len(demo_boxes)):\n    for j in range(i+1, len(demo_boxes)):\n        iou = calculate_iou(demo_boxes[i], demo_boxes[j])\n        print(f\"  IoU(Box{i+1}, Box{j+1}) = {iou:.3f}\")\n\n# Применение NMS\nselected = non_max_suppression(demo_boxes, demo_scores, iou_threshold=0.5)\nprint(f\"\\nПосле NMS (threshold=0.5): оставлен Box {selected[0]+1} (score={demo_scores[selected[0]]:.2f})\")\n\n# Визуализация\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\ncolors = ['red', 'blue', 'green', 'orange']\n\nfor ax_idx, (title, show_selected) in enumerate([\n    (\"Исходные прямоугольники\", False),\n    (\"После NMS (threshold=0.5)\", True)\n]):\n    ax = axes[ax_idx]\n    \n    for i, (box, score) in enumerate(zip(demo_boxes, demo_scores)):\n        x1, y1, x2, y2 = box\n        \n        if show_selected:\n            if i in selected:\n                rect = patches.Rectangle(\n                    (x1, y1), x2-x1, y2-y1,\n                    linewidth=3, edgecolor=colors[i], facecolor=colors[i],\n                    alpha=0.5, label=f'Box {i+1}: {score:.2f}'\n                )\n            else:\n                rect = patches.Rectangle(\n                    (x1, y1), x2-x1, y2-y1,\n                    linewidth=1, edgecolor='gray', facecolor='gray',\n                    alpha=0.2, linestyle='--'\n                )\n        else:\n            rect = patches.Rectangle(\n                (x1, y1), x2-x1, y2-y1,\n                linewidth=2, edgecolor=colors[i], facecolor=colors[i],\n                alpha=0.4, label=f'Box {i+1}: {score:.2f}'\n            )\n        \n        ax.add_patch(rect)\n        \n        # Номер в центре\n        cx, cy = (x1+x2)/2 - 5, (y1+y2)/2\n        if not show_selected or i in selected:\n            ax.text(cx, cy, str(i+1), fontsize=12, fontweight='bold', ha='center')\n\n    ax.set_xlim(0, 200)\n    ax.set_ylim(0, 200)\n    ax.set_title(title, fontsize=14)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.grid(True, alpha=0.3)\n    ax.legend(loc='upper left', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nВывод: NMS оставляет только Box с максимальным score среди перекрывающихся\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "saHpSVB8JKE8",
    "outputId": "f22c316e-f885-449f-9dca-d7815ed90dc3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "######2 Обнаружение объектов с помощью предобученной модели.\n"
   ],
   "metadata": {
    "id": "ZCusF-c0JMdp"
   }
  },
  {
   "cell_type": "code",
   "source": "print(\"Загрузка модели Faster R-CNN...\")\nprint(f\"Устройство: {DEVICE}\")\n\n# Загрузка с современным API (weights вместо pretrained)\nmodel = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\nmodel = model.to(DEVICE)\nmodel.eval()\n\nprint(f\"Модель загружена успешно\")\nprint(f\"Количество классов: {model.roi_heads.box_predictor.cls_score.out_features}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GcO9ZO_JJOJh",
    "outputId": "211e4cf9-24b3-4947-eea9-f10f456a5339"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def load_image_from_url(url: str, timeout: int = 10) -> Optional[Image.Image]:\n    \"\"\"Загружает изображение по URL.\n    \n    Args:\n        url: URL изображения\n        timeout: Таймаут запроса в секундах\n        \n    Returns:\n        PIL Image или None при ошибке\n    \"\"\"\n    try:\n        response = requests.get(url, timeout=timeout)\n        response.raise_for_status()\n        return Image.open(BytesIO(response.content)).convert(\"RGB\")\n    except Exception as e:\n        print(f\"Ошибка загрузки {url}: {e}\")\n        return None\n\n\ndef transform_image(image: Image.Image) -> torch.Tensor:\n    \"\"\"Преобразует PIL Image в тензор для модели.\"\"\"\n    transform = T.Compose([T.ToTensor()])\n    return transform(image).unsqueeze(0).to(DEVICE)\n\n\n# URL тестовых изображений COCO\nIMAGE_URLS = [\n    \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n    \"http://images.cocodataset.org/val2017/000000000139.jpg\",\n    \"http://images.cocodataset.org/val2017/000000037777.jpg\",\n]\n\n# Альтернативные URL на случай недоступности\nFALLBACK_URLS = [\n    \"http://images.cocodataset.org/val2017/000000000285.jpg\",\n    \"http://images.cocodataset.org/val2017/000000000724.jpg\",\n]\n\nprint(\"Загрузка тестовых изображений...\")\nimages = []\ntransformed_images = []\n\nfor i, url in enumerate(IMAGE_URLS):\n    img = load_image_from_url(url)\n    if img is None and i < len(FALLBACK_URLS):\n        print(f\"Пробуем альтернативный URL...\")\n        img = load_image_from_url(FALLBACK_URLS[i])\n    \n    if img is not None:\n        images.append(img)\n        transformed_images.append(transform_image(img))\n        print(f\"Изображение {len(images)} загружено: {img.size}\")\n\nprint(f\"\\nЗагружено {len(images)} изображений\")\n\n# Детекция объектов\nprint(\"\\nВыполнение детекции...\")\nall_predictions = []\n\nwith torch.no_grad():\n    for i, img_tensor in enumerate(transformed_images):\n        prediction = model(img_tensor)\n        all_predictions.append(prediction[0])\n        n_objects = len(prediction[0]['boxes'])\n        print(f\"Изображение {i+1}: найдено {n_objects} потенциальных объектов\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9m73PhnKBos",
    "outputId": "e0687a3d-49e8-42f0-acb1-c93eaa7822e2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def visualize_detections(\n    image: Image.Image,\n    predictions: Dict,\n    threshold: float = DEFAULT_CONFIDENCE_THRESHOLD,\n    title: str = \"Результаты детекции\"\n) -> Tuple[plt.Figure, List[str]]:\n    \"\"\"Визуализирует результаты детекции объектов.\n    \n    Args:\n        image: Исходное изображение\n        predictions: Словарь с boxes, labels, scores\n        threshold: Порог уверенности\n        title: Заголовок графика\n        \n    Returns:\n        Кортеж (figure, список обнаруженных классов)\n    \"\"\"\n    keep = predictions['scores'] > threshold\n    boxes = predictions['boxes'][keep].cpu().numpy()\n    labels = predictions['labels'][keep].cpu().numpy()\n    scores = predictions['scores'][keep].cpu().numpy()\n\n    fig, ax = plt.subplots(1, figsize=(14, 10))\n    ax.imshow(image)\n\n    cmap = plt.cm.get_cmap('tab20', len(COCO_CATEGORIES))\n    detected_objects = []\n\n    for box, label, score in zip(boxes, labels, scores):\n        if label >= len(COCO_CATEGORIES):\n            continue\n\n        class_name = COCO_CATEGORIES[label]\n        color = cmap(label)\n\n        x1, y1, x2, y2 = box\n        rect = patches.Rectangle(\n            (x1, y1), x2 - x1, y2 - y1,\n            linewidth=2, edgecolor=color, facecolor='none'\n        )\n        ax.add_patch(rect)\n\n        text = f\"{class_name}: {score:.2f}\"\n        ax.text(\n            x1, y1 - 8, text,\n            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.8),\n            fontsize=9, color='white', fontweight='bold'\n        )\n        detected_objects.append(class_name)\n\n    ax.axis('off')\n    unique = len(set(detected_objects))\n    ax.set_title(\n        f\"{title}\\nОбъектов: {len(boxes)} | Уникальных классов: {unique} | Порог: {threshold}\",\n        fontsize=14, fontweight='bold', pad=20\n    )\n\n    plt.tight_layout()\n    return fig, detected_objects\n\n\n# Визуализация результатов\nprint(\"Визуализация результатов детекции:\")\nprint(\"=\" * 70)\n\nfor i, (img, pred) in enumerate(zip(images, all_predictions)):\n    fig, detected = visualize_detections(img, pred, threshold=0.5, title=f\"Изображение {i+1}\")\n    plt.show()\n\n    keep = pred['scores'] > 0.5\n    scores = pred['scores'][keep].cpu().numpy()\n    \n    print(f\"\\nИзображение {i+1}:\")\n    print(f\"  Размер: {img.size}\")\n    print(f\"  Обнаружено объектов: {len(detected)}\")\n    print(f\"  Уникальных классов: {len(set(detected))}\")\n    if len(scores) > 0:\n        print(f\"  Уверенность: min={scores.min():.3f}, max={scores.max():.3f}, mean={scores.mean():.3f}\")\n    print(\"-\" * 70)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Uv5SmCgQRNp9",
    "outputId": "87fad150-5f74-41e6-c4c3-1edfc39f59f8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def analyze_threshold_effects(\n    image: Image.Image,\n    predictions: Dict,\n    image_num: int,\n    thresholds: List[float] = [0.9, 0.7, 0.5, 0.3]\n) -> Dict:\n    \"\"\"Анализирует влияние порога уверенности на результаты детекции.\n    \n    Args:\n        image: Исходное изображение\n        predictions: Результаты детекции\n        image_num: Номер изображения\n        thresholds: Список порогов для анализа\n        \n    Returns:\n        Словарь со статистикой по каждому порогу\n    \"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    axes = axes.ravel()\n    \n    cmap = plt.cm.get_cmap('tab20', len(COCO_CATEGORIES))\n    stats = {}\n\n    for idx, threshold in enumerate(thresholds):\n        ax = axes[idx]\n        \n        keep = predictions['scores'] > threshold\n        boxes = predictions['boxes'][keep].cpu().numpy()\n        labels = predictions['labels'][keep].cpu().numpy()\n        scores = predictions['scores'][keep].cpu().numpy()\n\n        stats[threshold] = {\n            'count': len(boxes),\n            'scores': scores,\n            'labels': labels\n        }\n\n        ax.imshow(image)\n\n        for box, label, score in zip(boxes, labels, scores):\n            if label >= len(COCO_CATEGORIES):\n                continue\n\n            color = cmap(label)\n            x1, y1, x2, y2 = box\n            rect = patches.Rectangle(\n                (x1, y1), x2-x1, y2-y1,\n                linewidth=1.5, edgecolor=color, facecolor='none'\n            )\n            ax.add_patch(rect)\n\n            if threshold <= 0.5:\n                class_name = COCO_CATEGORIES[label]\n                ax.text(\n                    x1, y1-6, f\"{class_name}: {score:.2f}\",\n                    bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=color, alpha=0.7),\n                    fontsize=7, color='white'\n                )\n\n        ax.axis('off')\n        unique = len(set([COCO_CATEGORIES[l] for l in labels if l < len(COCO_CATEGORIES)]))\n        ax.set_title(f\"Порог: {threshold}\\nОбъектов: {len(boxes)} | Классов: {unique}\", fontsize=11)\n\n    plt.suptitle(f\"Изображение {image_num}: Влияние порога уверенности\", fontsize=16, y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n    return stats\n\n\n# Анализ влияния порога\nprint(\"Исследование влияния порога уверенности:\")\nprint(\"=\" * 80)\n\nall_stats = []\nfor i, (img, pred) in enumerate(zip(images, all_predictions)):\n    print(f\"\\nАнализ изображения {i+1}\")\n    stats = analyze_threshold_effects(img, pred, i+1)\n    all_stats.append(stats)\n\n    print(f\"  Статистика по порогам:\")\n    for threshold in [0.9, 0.7, 0.5, 0.3]:\n        count = stats[threshold]['count']\n        if count > 0:\n            scores = stats[threshold]['scores']\n            print(f\"    Порог {threshold}: {count} объектов, уверенность {scores.min():.3f}-{scores.max():.3f}\")\n        else:\n            print(f\"    Порог {threshold}: нет объектов\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FxZ385h7Stib",
    "outputId": "c070afb8-101b-46dd-e673-e3c0b647935a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Сводный анализ\nprint(\"\\n\" + \"=\" * 80)\nprint(\"СВОДНЫЙ АНАЛИЗ ВЛИЯНИЯ ПОРОГА УВЕРЕННОСТИ\")\nprint(\"=\" * 80)\n\nfor img_num, stats in enumerate(all_stats, 1):\n    print(f\"\\nИзображение {img_num}:\")\n    \n    counts = [stats[t]['count'] for t in [0.9, 0.7, 0.5, 0.3]]\n    print(f\"  Количество объектов: 0.9→{counts[0]} | 0.7→{counts[1]} | 0.5→{counts[2]} | 0.3→{counts[3]}\")\n\n    if counts[0] > 0:\n        increase = ((counts[2] - counts[0]) / counts[0]) * 100\n        print(f\"  Прирост объектов (0.9→0.5): +{increase:.1f}%\")\n\n    if stats[0.5]['count'] > 0:\n        low_conf = sum(1 for s in stats[0.5]['scores'] if s < 0.7)\n        if low_conf > 0:\n            print(f\"  Объекты с уверенностью <0.7 при пороге 0.5: {low_conf}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmt5jXpHS1xv",
    "outputId": "87bdd936-9298-4028-b691-192077d6ae3f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Детальный анализ первого изображения\nif len(all_stats) > 0:\n    print(\"\\nДетальный анализ первого изображения:\")\n    print(\"=\" * 60)\n\n    img1_stats = all_stats[0]\n    t90 = img1_stats[0.9]\n    t50 = img1_stats[0.5]\n\n    print(f\"\\nСравнение порогов 0.9 и 0.5:\")\n    print(f\"  Порог 0.9: {t90['count']} объектов\")\n    print(f\"  Порог 0.5: {t50['count']} объектов\")\n\n    if t90['count'] > 0 and t50['count'] > 0:\n        labels_90 = set(t90['labels'])\n        labels_50 = set(t50['labels'])\n\n        classes_90 = [COCO_CATEGORIES[l] for l in labels_90 if l < len(COCO_CATEGORIES)]\n        print(f\"\\n  Классы при пороге 0.9: {classes_90}\")\n\n        new_labels = labels_50 - labels_90\n        if new_labels:\n            new_classes = [COCO_CATEGORIES[l] for l in new_labels if l < len(COCO_CATEGORIES)]\n            print(f\"  Новые классы при пороге 0.5: {new_classes}\")\n\n            # Объекты с низкой уверенностью\n            low_conf = [(COCO_CATEGORIES[l], s) for l, s in zip(t50['labels'], t50['scores'])\n                       if l in new_labels and s < 0.7 and l < len(COCO_CATEGORIES)]\n            if low_conf:\n                print(f\"  Объекты с низкой уверенностью (<0.7):\")\n                for name, score in low_conf:\n                    print(f\"    {name}: {score:.3f}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKcjNKhTS4Tb",
    "outputId": "78bf7f72-12e2-47a3-b6b6-e399f3c883c6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Отчёт по исследованию влияния порога уверенности на результаты детектирования**\n",
    "\n",
    "1. Количественный анализ\n",
    "\n",
    "Изображение 1:\n",
    "\n",
    "Порог 0.9: 3 объекта\n",
    "\n",
    "Порог 0.7: 4 объекта (+33%)\n",
    "\n",
    "Порог 0.5: 7 объектов (+133% от порога 0.9)\n",
    "\n",
    "Порог 0.3: 7 объекта (стабилизация)\n",
    "\n",
    "Изображение 2:\n",
    "\n",
    "Порог 0.9: 11 объектов\n",
    "\n",
    "Порог 0.7: 19 объектов (+73%)\n",
    "\n",
    "Порог 0.5: 25 объектов (+127% от порога 0.9)\n",
    "\n",
    "Порог 0.3: 36 объектов (+227% от порога 0.9)\n",
    "\n",
    "Изображение 3:\n",
    "\n",
    "Порог 0.9: 6 объектов\n",
    "\n",
    "Порог 0.7: 12 объектов (+100%)\n",
    "\n",
    "Порог 0.5: 17 объектов (+183% от порога 0.9)\n",
    "\n",
    "Порог 0.3: 29 объектов (+383% от порога 0.9)\n",
    "\n",
    "2. Качественный анализ по изображениям\n",
    "\n",
    "Изображение 1 (с кошками):\n",
    "\n",
    "Обнаружено при пороге 0.9: dog, clock\n",
    "Новые объекты при пороге 0.5: mouse, tv\n",
    "Объекты с низкой уверенностью (<0.7):\n",
    "\n",
    "tv: 0.541\n",
    "\n",
    "tv: 0.540\n",
    "\n",
    "mouse: 0.538\n",
    "\n",
    "Наблюдения: При снижении порога с 0.9 до 0.5 появились детекции объектов с уверенностью около 0.54. Вероятно, это ложные срабатывания, так как на изображении с кошками вряд ли присутствуют телевизоры и мыши.\n",
    "\n",
    "Изображение 2 (уличная сцена):\n",
    "\n",
    "При пороге 0.5 обнаружено 6 объектов с уверенностью <0.7. Это указывает на значительное количество детекций с низкой достоверностью при использовании низкого порога.\n",
    "\n",
    "Изображение 3 (уличная сцена 2):\n",
    "\n",
    "При пороге 0.5 обнаружено 5 объектов с уверенностью <0.7. Наибольший относительный прирост объектов при снижении порога (+183%).\n",
    "\n",
    "3. Основные закономерности\n",
    "\n",
    "Экспоненциальный рост количества детекций: При снижении порога от 0.9 до 0.3 количество обнаруженных объектов увеличивается в 2-4 раза.\n",
    "Насыщение детекций: Для Изображения 1 количество объектов стабилизировалось на пороге 0.5, что может указывать на ограниченное количество реальных объектов на сцене.\n",
    "Пороговый эффект: Наиболее значительный прирост детекций происходит при снижении порога от 0.7 до 0.5.\n",
    "Процент низкодостоверных детекций:\n",
    "\n",
    "Изображение 1: 3 из 7 объектов (43%) имеют уверенность <0.7\n",
    "\n",
    "Изображение 2: 6 из 25 объектов (24%) имеют уверенность <0.7\n",
    "\n",
    "Изображение 3: 5 из 17 объектов (29%) имеют уверенность <0.7\n",
    "4. Выводы о влиянии порога уверенности\n",
    "\n",
    "При высоком пороге (0.9):\n",
    "\n",
    "Обнаруживаются только наиболее уверенные объекты\n",
    "Минимальное количество ложных срабатываний\n",
    "Пропускается значительная часть объектов (особенно в сложных сценах)\n",
    "Для Изображения 3 пропускается 65% объектов, которые детектируются при пороге 0.5\n",
    "При низком пороге (0.5):\n",
    "\n",
    "Значительно увеличивается полнота детекции (+127-183%)\n",
    "Появляется много детекций с низкой уверенностью (24-43% от общего количества)\n",
    "Высока вероятность ложных срабатываний\n",
    "Визуализация становится перегруженной\n",
    "5. Рекомендации\n",
    "\n",
    "Для критически важных систем (безопасность, медицина): использовать порог 0.8-0.9\n",
    "Для общего применения: оптимальный порог 0.7 (баланс точности и полноты)\n",
    "Для максимального покрытия: порог 0.5 с обязательной постобработкой (NMS)\n",
    "Для исследовательских целей: порог 0.3-0.5 для выявления всех возможных детекций\n",
    "6. Общий вывод\n",
    "\n",
    "Выбор порога уверенности представляет собой классический компромисс между точностью и полнотой детекции. Более высокие пороги обеспечивают высокую точность за счет пропуска объектов, в то время как низкие пороги максимизируют полноту за счет увеличения ложных срабатываний. Для большинства практических задач рекомендуется порог 0.7 как оптимальный баланс."
   ],
   "metadata": {
    "id": "WNZ8c5dtTekh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3"
   ],
   "metadata": {
    "id": "HMokeHTcVOLK"
   }
  },
  {
   "cell_type": "code",
   "source": "print(\"Загрузка моделей сегментации...\")\n\n# Семантическая сегментация (DeepLabv3)\nsemantic_model = deeplabv3_resnet101(weights=DeepLabV3_ResNet101_Weights.DEFAULT)\nsemantic_model = semantic_model.to(DEVICE)\nsemantic_model.eval()\nprint(\"DeepLabv3 загружен\")\n\n# Поэкземплярная сегментация (Mask R-CNN)\ninstance_model = maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT)\ninstance_model = instance_model.to(DEVICE)\ninstance_model.eval()\nprint(\"Mask R-CNN загружен\")\n\n\ndef get_color_palette(num_classes: int) -> np.ndarray:\n    \"\"\"Создаёт цветовую палитру для визуализации сегментации.\"\"\"\n    np.random.seed(42)\n    palette = np.random.randint(0, 255, size=(num_classes, 3))\n    palette[0] = [0, 0, 0]  # Фон - чёрный\n    return palette\n\n\ncolor_palette = get_color_palette(len(COCO_CATEGORIES))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fuQZf2_VPhO",
    "outputId": "e8c0c725-7099-4563-eeeb-b5a51cfcbde0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def preprocess_for_segmentation(image: Image.Image, size: int = 520) -> torch.Tensor:\n    \"\"\"Подготавливает изображение для моделей сегментации.\"\"\"\n    transform = T.Compose([\n        T.Resize(size),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    return transform(image).unsqueeze(0).to(DEVICE)\n\n\n# Загрузка тестовых изображений для сегментации\nSEGMENTATION_URLS = [\n    \"http://images.cocodataset.org/val2017/000000397133.jpg\",\n    \"http://images.cocodataset.org/val2017/000000037777.jpg\",\n]\n\nprint(\"Загрузка изображений для сегментации...\")\ntest_images = []\nfor url in SEGMENTATION_URLS:\n    img = load_image_from_url(url)\n    if img is not None:\n        test_images.append(img)\n        print(f\"Загружено: {img.size}\")\n\n# Применение моделей\nprint(\"\\nПрименение семантической сегментации...\")\nsemantic_results = []\nfor img in test_images:\n    input_tensor = preprocess_for_segmentation(img)\n    with torch.no_grad():\n        output = semantic_model(input_tensor)['out'][0]\n        semantic_results.append(output.cpu().numpy())\n\nprint(\"Применение поэкземплярной сегментации...\")\ninstance_results = []\nfor img in test_images:\n    input_tensor = preprocess_for_segmentation(img, size=800)\n    with torch.no_grad():\n        output = instance_model(input_tensor)[0]\n        instance_results.append(output)\n\nprint(\"Сегментация выполнена!\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlDOCDk2VgQi",
    "outputId": "a7ee1694-064d-4fcd-abf0-30f0f8e29945"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3(b) Результаты применения моделей**\n",
    "\n",
    "Семантическая сегментация (DeepLabv3):\n",
    "\n",
    "- Модель успешно применена к тестовым изображениям\n",
    "- Получены карты классов (masks) размерностью [C, H, W], где C - количество классов\n",
    "- Для каждого пикселя определён наиболее вероятный класс\n",
    "\n",
    "Поэкземплярная сегментация (Mask R-CNN):\n",
    "\n",
    "Модель применена к тем же изображениям\n",
    " - Получены наборы масок для каждого обнаруженного объекта\n",
    " - Каждому объекту соответствует bounding box, класс и маска сегментации\n",
    " - Уверенность детекции варьируется в зависимости от объекта"
   ],
   "metadata": {
    "id": "hZthoR3wWZ1m"
   }
  },
  {
   "cell_type": "code",
   "source": "def compare_segmentation_methods(image_idx: int) -> Tuple[set, dict]:\n    \"\"\"Сравнивает семантическую и поэкземплярную сегментацию.\n    \n    Args:\n        image_idx: Индекс изображения\n        \n    Returns:\n        Кортеж (классы семантической сегм., словарь классов instance сегм.)\n    \"\"\"\n    img = test_images[image_idx]\n    sem_result = semantic_results[image_idx]\n    inst_result = instance_results[image_idx]\n\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n    # Оригинал\n    axes[0, 0].imshow(img)\n    axes[0, 0].set_title(\"Исходное изображение\", fontsize=12, fontweight='bold')\n    axes[0, 0].axis('off')\n\n    # Семантическая сегментация\n    predictions = sem_result.argmax(0)\n    colored_mask = np.zeros((*predictions.shape, 3), dtype=np.uint8)\n    for class_idx in range(len(COCO_CATEGORIES)):\n        mask = predictions == class_idx\n        if mask.any():\n            colored_mask[mask] = color_palette[class_idx]\n\n    colored_img = Image.fromarray(colored_mask).resize(img.size, Image.NEAREST)\n    sem_overlay = Image.blend(img.convert('RGBA'), colored_img.convert('RGBA'), alpha=0.5)\n    axes[0, 1].imshow(sem_overlay)\n    axes[0, 1].set_title(\"Семантическая сегментация (DeepLabv3)\", fontsize=12, fontweight='bold')\n    axes[0, 1].axis('off')\n\n    # Instance сегментация\n    keep = inst_result['scores'] > 0.5\n    masks = inst_result['masks'][keep].cpu().numpy()\n    labels = inst_result['labels'][keep].cpu().numpy()\n\n    instance_overlay = np.array(img).astype(np.float32) / 255.0\n    instance_cmap = plt.cm.get_cmap('tab20', len(masks) + 1)\n\n    for i, (mask, label) in enumerate(zip(masks, labels)):\n        if label >= len(COCO_CATEGORIES):\n            continue\n\n        mask_resized = (mask[0] > 0.5).astype(np.uint8)\n        mask_resized = np.array(Image.fromarray(mask_resized * 255).resize(img.size, Image.NEAREST)) / 255.0\n\n        color = np.array(instance_cmap(i))[:3]\n        mask_3d = np.stack([mask_resized] * 3, axis=2)\n        instance_overlay = np.where(mask_3d > 0.5, 0.3 * instance_overlay + 0.7 * color, instance_overlay)\n\n    axes[1, 0].imshow(instance_overlay)\n    axes[1, 0].set_title(\"Поэкземплярная сегментация (Mask R-CNN)\", fontsize=12, fontweight='bold')\n    axes[1, 0].axis('off')\n\n    # Сравнение\n    comparison = np.zeros((img.size[1], img.size[0] * 2, 3))\n    comparison[:, :img.size[0]] = np.array(sem_overlay)[:, :, :3] / 255.0\n    comparison[:, img.size[0]:] = instance_overlay\n\n    axes[1, 1].imshow(comparison)\n    axes[1, 1].set_title(\"Сравнение: слева — семантическая, справа — поэкземплярная\", fontsize=12)\n    axes[1, 1].axis('off')\n\n    plt.suptitle(f\"Сравнение методов сегментации (Изображение {image_idx+1})\", fontsize=16, y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n    # Статистика\n    sem_classes = set()\n    for class_idx in np.unique(predictions):\n        if class_idx < len(COCO_CATEGORIES):\n            sem_classes.add(COCO_CATEGORIES[class_idx])\n\n    inst_classes = {}\n    for label in labels:\n        if label < len(COCO_CATEGORIES):\n            name = COCO_CATEGORIES[label]\n            inst_classes[name] = inst_classes.get(name, 0) + 1\n\n    return sem_classes, inst_classes\n\n\n# Сравнительный анализ\nprint(\"\\nСравнительный анализ методов сегментации:\")\nprint(\"=\" * 60)\n\nfor i in range(len(test_images)):\n    sem_classes, inst_classes = compare_segmentation_methods(i)\n\n    print(f\"\\nИзображение {i+1}:\")\n    print(f\"  Семантическая сегментация:\")\n    for cls in sorted(sem_classes):\n        print(f\"    - {cls}\")\n\n    print(f\"  Поэкземплярная сегментация:\")\n    for cls, count in sorted(inst_classes.items()):\n        print(f\"    - {cls}: {count} экземпляров\")\n\n    common = set(sem_classes) & set(inst_classes.keys())\n    print(f\"  Общих классов: {len(common)}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WX8gDbhxWvkA",
    "outputId": "a9756a4b-d712-47d3-da0a-11a53be34381"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "3(d) Сравнение семантической и поэкземплярной сегментации\n",
    "\n",
    "Результаты сравнения методов\n",
    "\n",
    "Изображение 1\n",
    "\n",
    "Семантическая сегментация обнаружила: background, bird, cat, fire hydrant\n",
    "Поэкземплярная сегментация обнаружила: banana (1 экземпляр), broccoli (1), keyboard (1), person (1)\n",
    "Общих классов: 0\n",
    "Изображение 2\n",
    "\n",
    "Семантическая сегментация обнаружила: background, boat, fire hydrant, horse\n",
    "Поэкземплярная сегментация обнаружила: traffic light (1 экземпляр)\n",
    "Общих классов: 0\n",
    "Отличия семантической и поэкземплярной сегментации\n",
    "\n",
    "Семантическая сегментация\n",
    "\n",
    "Присваивает класс каждому пикселю изображения\n",
    "Не различает отдельные экземпляры объектов одного класса\n",
    "Все объекты одного класса окрашиваются одинаковым цветом\n",
    "Объекты одного класса сливаются в единую область\n",
    "Подходит для задач, где важно знать что находится на изображении, но не сколько\n",
    "Поэкземплярная сегментация\n",
    "\n",
    "Выделяет каждый объект как отдельную сущность\n",
    "Каждый экземпляр получает уникальную маску и bounding box\n",
    "Разные экземпляры одного класса окрашиваются разными цветами\n",
    "Позволяет подсчитывать количество объектов\n",
    "Подходит для задач, где важно знать сколько и где именно находятся объекты\n",
    "Как семантическая сегментация отображает объекты\n",
    "\n",
    "Когда в сцене присутствует несколько объектов одного класса, семантическая сегментация объединяет их в единую область. Все пиксели, принадлежащие одному классу, окрашиваются одним цветом без разделения на отдельных животных. Границы между экземплярами размываются или игнорируются.\n",
    "\n",
    "Как instance-сегментация выделяет каждый экземпляр отдельно\n",
    "\n",
    "Instance-сегментация сначала детектирует каждый объект отдельно, а затем создает для него уникальную маску. Даже если объекты одного класса перекрываются, каждый из них получает свою собственную маску и идентификатор. Это позволяет точно определять границы каждого экземпляра и отслеживать их по отдельности.\n",
    "\n",
    "Почему для поэкземплярной сегментации требуется сочетание методов детектирования и сегментации\n",
    "\n",
    "Детектирование необходимо для идентификации отдельных объектов и определения их приблизительного местоположения\n",
    "Сегментация требуется для точного выделения границ каждого объекта на пиксельном уровне\n",
    "Комбинация этих методов позволяет сначала найти объекты, а затем точно выделить их контуры\n",
    "Без детектирования невозможно было бы различать отдельные экземпляры объектов одного класса\n",
    "Выводы по эксперименту\n",
    "\n",
    "Качественные различия моделей: Разные модели обнаружили совершенно разные классы объектов, что указывает на разную специализацию и обучение моделей\n",
    "Практическая применимость: Выбор метода зависит от задачи. Для анализа сцен подходит семантическая сегментация, для подсчета объектов необходима поэкземплярная\n",
    "Точность моделей: Результаты показывают, что обе модели имеют ограничения и могут давать разные результаты на одних и тех же изображениях\n",
    "Визуальное представление: Instance-сегментация предоставляет более информативную визуализацию, показывая не только классы объектов, но и их количество и расположение\n",
    "Рекомендации: Для сложных задач, требующих работы с отдельными объектами, следует использовать поэкземплярную сегментацию несмотря на ее большую вычислительную сложность"
   ],
   "metadata": {
    "id": "USNyoErUXkGb"
   }
  }
 ]
}